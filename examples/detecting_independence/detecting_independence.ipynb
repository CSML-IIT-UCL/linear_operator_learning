{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Independence\n",
    "\n",
    "In *Neural Conditional Probability for Uncertainty Quantification* (Kostic et al., 2024), the authors claim that the (deflated) conditional expectation operator can be used to detect the independence of two random variables X and Y by verifying whether it is zero. Here, we show this equivaliance in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "def make_dataset(n_samples: int = 200, t: float = 0.0):\n",
    "    \"\"\"Draw sample from data model Y = tX + (1-t)X_, where X and X_ are independent gaussians.\n",
    "\n",
    "    If t = 0, then X and Y are independent. Otherwise, if t->1, X and Y become ever more dependent.\n",
    "\n",
    "    Args:\n",
    "        n_samples (int, optional): Number of samples. Defaults to 200.\n",
    "        t (float, optional): Interpolation factor. Defaults to 0.0.\n",
    "    \"\"\"\n",
    "    X = torch.normal(mean=0, std=1, size=(n_samples, 1))\n",
    "    X_ = torch.normal(mean=0, std=1, size=(n_samples, 1))\n",
    "    Y = t * X + (1 - t) * X_\n",
    "\n",
    "    ds = TensorDataset(X, Y)\n",
    "\n",
    "    # Split data into train and val sets\n",
    "    train_ds, val_ds = random_split(ds, lengths=[0.85, 0.15])\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for training NCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.nn import Module\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from linear_operator_learning.nn import MLP, NCP, L2ContrastiveLoss\n",
    "from linear_operator_learning.nn.functional import orthonormality_regularization\n",
    "\n",
    "\n",
    "class NCPTrainingModule(L.LightningModule):\n",
    "    \"\"\"Optional LightningModule for training NCP. It isn't necessary to use lightning to run NCP!\n",
    "\n",
    "    Args:\n",
    "        ncp (NCP): NCP Module. See modules/ncp.py for more details.\n",
    "        loss (Module, optional): Loss function for training NCP. Defauls to L2ContrastiveLoss,\n",
    "            the loss used in the paper.\n",
    "        loss_kwargs (dict, optional): Keyword arguments to be passed to the loss. Defaults to dict(),\n",
    "            as the L2ContrastiveLoss doesn't implement regularization.\n",
    "        gamma (float, optional): Orthonormality regularization strength. Defauls to 1e-3.\n",
    "        optimizer (Optimizer, optional): Torch optimizer for optimizing NCP. Defaults to Adam.\n",
    "        optimizer_kwargs (dict, optional): Keyword arguments to be passed to the optimizer.\n",
    "            Defaults to {\"lr\": 5e-4}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Hack to store the results of different runs without heavy machinery.\n",
    "        results: dict,\n",
    "        run_id: tuple,\n",
    "        # NCP training interface begins here:\n",
    "        ncp: NCP,\n",
    "        loss: Module = L2ContrastiveLoss,\n",
    "        loss_kwargs: dict = dict(),\n",
    "        gamma: float = 1e-3,\n",
    "        optimizer: Optimizer = Adam,\n",
    "        optimizer_kwargs: dict = {\"lr\": 5e-4},\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.results = results\n",
    "        self.run_id = run_id\n",
    "        self.ncp = ncp\n",
    "        self.loss = loss(**loss_kwargs)\n",
    "        self.gamma = gamma\n",
    "        self._optimizer = optimizer\n",
    "        self._optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self._optimizer(self.parameters(), **self._optimizer_kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        u, v = self.ncp(*batch)\n",
    "\n",
    "        Dr = self.ncp.S.weights  # TODO: Is there a better name?\n",
    "        _loss = self.loss(u, (Dr @ v.T).T)  # Dr @ v\n",
    "\n",
    "        # TODO: Pass params\n",
    "        # reg = orthonormality_regularization()\n",
    "        reg = 0\n",
    "\n",
    "        loss = _loss + self.gamma * reg\n",
    "\n",
    "        self.log(\"loss/train\", loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        u, v = self.ncp(*batch)\n",
    "\n",
    "        Dr = self.ncp.S.weights  # TODO: Is there a better name?\n",
    "        _loss = self.loss(u, (Dr @ v.T).T)  # Dr @ v\n",
    "\n",
    "        # TODO: Pass params\n",
    "        # reg = orthonormality_regularization()\n",
    "        reg = 0\n",
    "\n",
    "        loss = _loss + self.gamma * reg\n",
    "\n",
    "        self.log(\"loss/val\", loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        \"\"\"Perform whitening. In real-world applications, this would use the entire training set.\"\"\"\n",
    "        WHITENING_N_SAMPLES = 2000\n",
    "        t = self.run_id[0]\n",
    "        x = torch.normal(mean=0, std=1, size=(WHITENING_N_SAMPLES, 1))\n",
    "        x_ = torch.normal(mean=0, std=1, size=(WHITENING_N_SAMPLES, 1))\n",
    "        y = t * x + (1 - t) * x_\n",
    "        self.ncp._update_whitening_buffers(x=x, y=y)\n",
    "        self.results[self.run_id] = self.ncp.sing_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "RUN_PATH = Path(\"runs\")\n",
    "RUN_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "SEED = 1\n",
    "REPEATS = 1\n",
    "BATCH_SIZE = 256\n",
    "N_SAMPLES = 5000\n",
    "NCP_PARAMS = dict(\n",
    "    output_shape=2,\n",
    "    n_hidden=2,\n",
    "    layer_size=32,\n",
    "    activation=torch.nn.ELU,\n",
    "    bias=False,\n",
    "    iterative_whitening=False,\n",
    ")\n",
    "\n",
    "results = dict()\n",
    "for t in torch.linspace(start=0, end=1, steps=11):\n",
    "    for r in range(REPEATS):\n",
    "        run_id = (round(t.item(), 2), r)\n",
    "        print(f\"run_id = {run_id}\")\n",
    "\n",
    "        # Load data_________________________________________________________________________________\n",
    "        seed_everything(seed=SEED)\n",
    "        train_ds, val_ds = make_dataset(n_samples=N_SAMPLES, t=t.item())\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Build NCP_________________________________________________________________________________\n",
    "        ncp = NCP(\n",
    "            embedding_x=MLP(input_shape=1, **NCP_PARAMS),\n",
    "            embedding_dim_x=NCP_PARAMS[\"output_shape\"],\n",
    "            embedding_y=MLP(input_shape=1, **NCP_PARAMS),\n",
    "            embedding_dim_y=NCP_PARAMS[\"output_shape\"],\n",
    "        )\n",
    "\n",
    "        # Train NCP_________________________________________________________________________________\n",
    "        # Training module for lightning\n",
    "        model = NCPTrainingModule(results=results, run_id=run_id, ncp=ncp)\n",
    "\n",
    "        # Create logger\n",
    "        logger = CSVLogger(save_dir=RUN_PATH, name=\"detecting_independence\", version=run_id)\n",
    "\n",
    "        # Create callbacks\n",
    "        # TODO: Add ModelCheckpoint and EarlyStopping\n",
    "        # ckpt_call = ModelCheckpoint()\n",
    "        # early_call = EarlyStopping()\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=\"cpu\",\n",
    "            precision=\"bf16\",\n",
    "            logger=logger,\n",
    "            # callbacks=[ckpt_call, early_call],\n",
    "            max_epochs=100,\n",
    "            check_val_every_n_epoch=25,\n",
    "            enable_model_summary=False,\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    data=[(t, r, svals.max().item()) for ((t, r), svals) in results.items()],\n",
    "    columns=[\"t\", \"r\", \"norm\"],\n",
    ")\n",
    "sns.pointplot(results_df, x=\"t\", y=\"norm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
