{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Independence\n",
    "\n",
    "In the paper \"Neural Conditional Probability for Uncertainty Quantification\" by Kostic et al., it is claimed that the conditional expectation operator may be used for detecting the independence of two random variables by checking if it is zero. Here, we show this equivaliance in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "def make_dataset(n_samples: int = 200, t: float = 0.0):\n",
    "    \"\"\"Draw sample from data model Y = tX + (1-t)X_, where X and X_ are independent gaussians.\"\"\"\n",
    "    X = torch.normal(mean=0, std=1, size=(n_samples, 1))\n",
    "    X_ = torch.normal(mean=0, std=1, size=(n_samples, 1))\n",
    "    Y = t * X + (1 - t) * X_\n",
    "\n",
    "    ds = TensorDataset(X, Y)\n",
    "\n",
    "    # Split data into train and val sets\n",
    "    train_ds, val_ds = random_split(ds, [0.85, 0.15])\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "from linear_operator_learning.nn import MLP\n",
    "\n",
    "\n",
    "class NCP(Module):\n",
    "    \"\"\"Neural Conditional Probability in PyTorch.\n",
    "\n",
    "    Args:\n",
    "        embedding_x (Module): Neural embedding of x.\n",
    "        embedding_y (Module): Neural embedding of y.\n",
    "        matrix_form (str, optional): Either 'dense' of 'sparse'. Defaults to 'dense'.\n",
    "        learnable_matrix (bool, optional): Whether the matrix layer is learnable. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_x: Module,\n",
    "        embedding_y: Module,\n",
    "        matrix_form: str = \"dense\",\n",
    "        learnable_matrix: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.U = embedding_x\n",
    "        self.V = embedding_y\n",
    "        self.S = NCP.make_matrix_layer(matrix_form, learnable_matrix)\n",
    "\n",
    "        # Register buffers for the statistics of the latent variables u and v.\n",
    "        self._register_stats_buffers()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_matrix_layer(matrix_form: str, learnable_matrix: bool) -> Module:\n",
    "        \"\"\"Creates a module for the truncated operator's matrix form. See the class docs for the arguments.\"\"\"\n",
    "        if matrix_form != \"dense\" or not learnable_matrix:\n",
    "            raise NotImplementedError(\n",
    "                \"This NCP implementation only supports dense learnable matrix layer.\"\n",
    "            )\n",
    "\n",
    "    def _register_stats_buffers() -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.optim import Adam, Optimizer\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from linear_operator_learning.nn import L2ContrastiveLoss\n",
    "\n",
    "\n",
    "class NCPTrainingModule(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Hack to store the results of different runs without heavy machinery.\n",
    "        results: dict,\n",
    "        run_id: tuple,\n",
    "        # NCP training interface begins here:\n",
    "        ncp: NCP,\n",
    "        loss: Module = L2ContrastiveLoss,\n",
    "        loss_kwargs: dict = {\"gamma\": 1e-3},\n",
    "        optimizer: Optimizer = Adam,\n",
    "        optimizer_kwargs: dict = {\"lr\", 5e-4},\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ncp = ncp\n",
    "        self.loss = loss(**loss_kwargs)\n",
    "        self._optimizer = optimizer\n",
    "        self._optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self._optimizer(self.parameters, **self._optimizer_kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out = self.ncp(*batch)\n",
    "        loss = self.loss(*out)\n",
    "        self.log(\"loss/train\", loss, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out = self.ncp(*batch)\n",
    "        loss = self.loss(*out)\n",
    "        self.log(\"loss/val\", loss, prog_bar=False)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id = (0.0, 0)\n",
      "run_id = (0.1, 0)\n",
      "run_id = (0.2, 0)\n",
      "run_id = (0.3, 0)\n",
      "run_id = (0.4, 0)\n",
      "run_id = (0.5, 0)\n",
      "run_id = (0.6, 0)\n",
      "run_id = (0.7, 0)\n",
      "run_id = (0.8, 0)\n",
      "run_id = (0.9, 0)\n",
      "run_id = (1.0, 0)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lightning import seed_everything\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "RUN_PATH = Path(\"runs\")\n",
    "RUN_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "SEED = 1\n",
    "REPEATS = 1\n",
    "BATCH_SIZE = 2048\n",
    "N_SAMPLES = 10 * BATCH_SIZE\n",
    "NCP_PARAMS = dict(\n",
    "    output_shape=2,\n",
    "    n_hidden=2,\n",
    "    layer_size=32,\n",
    "    activation=torch.nn.ELU,\n",
    "    bias=False,\n",
    "    iterative_whitening=False,\n",
    ")\n",
    "\n",
    "results = dict()\n",
    "for t in torch.linspace(start=0, end=1, steps=11):\n",
    "    for r in range(REPEATS):\n",
    "        run_id = (round(t.item(), 2), r)\n",
    "        print(f\"run_id = {run_id}\")\n",
    "\n",
    "        # Load data_________________________________________________________________________________\n",
    "        seed_everything(seed=SEED)\n",
    "        train_ds, val_ds = make_dataset(n_samples=N_SAMPLES, t=t.item())\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        val_ds = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Build NCP_________________________________________________________________________________\n",
    "        ncp = NCP(\n",
    "            embedding_x=MLP(input_shape=1, **NCP_PARAMS),\n",
    "            embedding_y=MLP(input_shape=1, **NCP_PARAMS),\n",
    "        )\n",
    "\n",
    "        # Train NCP_________________________________________________________________________________\n",
    "        # Training module for lightning\n",
    "        model = NCPTrainingModule(results=results, run_id=run_id, ncp=ncp)\n",
    "\n",
    "        # Create logger\n",
    "        logger = CSVLogger(save_dir=RUN_PATH, name=\"detecting_independence\", version=run_id)\n",
    "\n",
    "        # Create callbacks\n",
    "        # TODO: Add ModelCheckpoint and EarlyStopping\n",
    "        # ckpt_call = ModelCheckpoint()\n",
    "        # early_call = EarlyStopping()\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            accelerator=\"cpu\",\n",
    "            precision=\"bf16\",\n",
    "            logger=logger,\n",
    "            # callbacks=[ckpt_call, early_call],\n",
    "            max_epochs=100,\n",
    "            check_val_every_n_epoch=25,\n",
    "            enable_model_summary=False,\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    data=[(t, r, norm) for ((t, r), norm) in results.items()], columns=[\"t\", \"r\", \"norm\"]\n",
    ")\n",
    "sns.pointplot(results_df, x=\"t\", y=\"norm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
